<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Planning from Point Clouds</title>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="./static/css/academicons.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
</head>

<body>


    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">


                        <h1 class="title is-1 publication-title">
                            <span style="color: #0066cc;">Planning from Point Clouds</span><br>
                            <span>over Continuous Actions for Multi-object Rearrangement</span>
                        </h1>

                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                <a target="_blank" href="https://kallol-saha.github.io/" style="font-size: 125%;">
                                    Kallol Saha</a><sup>*1</sup> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                                <a target="_blank" href="https://amburger66.github.io/" style="font-size: 125%;">
                                    Amber Li</a><sup>*1</sup> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                                <a target="_blank" href="https://www.linkedin.com/in/angela-rodriguez-izq/" style="font-size: 125%;">
                                    Angela Rodriguez-Izquierdo</a><sup>*2</sup> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                                <a target="_blank" href="https://www.linkedin.com/in/yu-lifan/" style="font-size: 125%;">
                                    Lifan Yu</a><sup>1</sup>
                                    <br>
                                <a target="_blank" href="https://beisner.me/" style="font-size: 125%;">
                                    Ben Eisner</a><sup>1</sup> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                                <a target="_blank" href="https://www.cs.cmu.edu/~maxim/" style="font-size: 125%;">
                                    Maxim Likhachev</a><sup>1</sup> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                                <a target="_blank" href="https://davheld.github.io/" style="font-size: 125%;">
                                    David Held</a><sup>1</sup>
                                <br>
                                <sup>*</sup>Equal Contribution
                                
                                <br>
                                <div style="margin: 10px 0;">
                                    <sup style="font-size: 120%; vertical-align: top;">1&nbsp;&nbsp;</sup><a href="https://www.ri.cmu.edu/" target="_blank"><img src="logos/cmu-ri.jpeg" alt="CMU Robotics Institute" style="height: 55px; vertical-align: middle;"></a> 
                                    <span style="display: inline-block; width: 80px;"></span>
                                    <sup style="font-size: 120%; vertical-align: top;">2&nbsp;&nbsp;</sup><a href="https://www.princeton.edu/" target="_blank"><img src="logos/pu-logo.svg" alt="Princeton University" style="height: 23px; vertical-align: middle;"></a>
                                </div>
    
                            </span>
                        </div>
    
                        <br>
                        
                        <div style="text-align: center;">
                            <h3 class="title is-4 conference-authors">
                                <a target="_blank" href="https://www.corl.org/home">
                                    Conference on Robot Learning (CoRL) 2025<br><span style="color: red;">(Oral Presentation - top 5.7%)</span>
                                </a>
                            </h3>
                        </div>

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <!-- TODO REPLACE ALL LINKS -->
                                <span class="link-block">
                                    <a target="_blank" href="https://planning-from-point-clouds.github.io/"
                                        class="external-link button is-normal is-rounded is-dark" style="font-size: 125%;">
                                        <span class="icon">
                                            <i class="ai ai-arxiv"></i>
                                        </span>
                                        <span>arXiv</span>
                                    </a>
                                </span>

                                <span class="link-block">
                                    <a target="_blank" href="https://github.com/kallol-saha/SPOT"
                                        class="external-link button is-normal is-rounded is-dark" style="font-size: 125%;">
                                        <span class="icon">
                                            <i class="fab fa-github"></i>
                                        </span>
                                        <span>Code</span>
                                    </a>
                                </span>
                            </div>

                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Teaser Figure -->
    <section class="section" style="padding: 0">
        <div class="container is-fluid">
            <!-- Abstract. -->
            <div class="columns is-centered has-text-centered">
                <video poster="" id="" autoplay muted controls loop width="100%" playbackRate=2.0 style="border-radius: 5px; max-width: 1300px;">
                    <source src="assets/website_teaser_video.mp4" type="video/mp4">
                </video>
            </div>
            <div class="columns is-centered has-text-centered" style="margin-top: 10px;">
                <p style="font-size: 130%; color: #666; font-style: italic;">(All real robot execution videos on this website are at 5x the speed of actual execution)</p>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <!-- Abstract. -->
            <div class="columns is-centered has-text-centered">
                <div class="column">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p style="font-size: 125%"><span style="font-weight: bold;">Long-horizon planning</span>  for robot manipulation is a challenging problem
                            that requires reasoning about the effects of a sequence of actions on a physical 3D scene. 
                            While traditional task planning methods are shown to be effective for long-horizon manipulation, 
                            they require discretizing the continuous state and action space into symbolic descriptions of 
                            objects, object relationships, and actions. Instead, we propose a hybrid learning-and-planning 
                            approach that leverages learned models as domain-specific priors to guide search in 
                            high-dimensional continuous action spaces. We introduce 
                            <span style="font-weight: bold;">SPOT</span>: 
                            <span style="font-weight: bold;">S</span>earch over <span style="font-weight: bold;">P</span>oint 
                            cloud <span style="font-weight: bold;">O</span>bject 
                            <span style="font-weight: bold;">T</span>ransformations, which plans by searching for a sequence 
                            of transformations from an initial scene point cloud to a goal-satisfying point cloud. SPOT 
                            samples candidate actions from learned suggesters that operate on partially observed point 
                            clouds, eliminating the need to discretize actions or object relationships. We evaluate SPOT on 
                            multi-object rearrangement tasks, reporting task planning success and task execution success in 
                            both simulation and real-world environments. Our experiments show that SPOT generates successful 
                            plans and outperforms a policy-learning approach. We also perform ablations that highlight the 
                            importance of search-based planning.</p>

                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- interactive examples of long horizon tasks -->
    <section class="section" style="padding-top: 1rem;">
        <div class="container" style="max-width: 75%; margin: 0 auto;">
            <div class="rows">
                <div class="rows is-centered">
                    <div class="row is-full-width">
                        <h2 class="title is-3 has-text-centered"><span class="dvima">Select the configuration you want to rearrange:</span></h2>
                        <!-- <p style="font-size: 125%">Please click an image below to view the visualizations of the policy outputs.</p> -->
                    </div>
                </div>
            </div>
            <br>
            
            <!-- First row with 7 images -->
            <div class="columns is-centered">
              <div class="column is-full-width" style="display: flex; flex-wrap: wrap; justify-content: center;">
                  <img src="assets/task_chooser/real_world_1.png" width="8%" height="auto" style="border-radius: 10px; margin: 5px; object-fit: cover; aspect-ratio: 1/1;" alt='Store an item' onclick="populateDemo(this, 1);">
                  <img src="assets/task_chooser/real_world_2.png" width="8%" height="auto" style="border-radius: 10px; margin: 5px; object-fit: cover; aspect-ratio: 1/1;" alt='Heat soup' onclick="populateDemo(this, 1);">
                  <img src="assets/task_chooser/real_world_3.png" width="8%" height="auto" style="border-radius: 10px; margin: 5px; object-fit: cover; aspect-ratio: 1/1;" alt='Slide Window' onclick="populateDemo(this, 1);">
                  <img src="assets/task_chooser/real_world_4.png" width="8%" height="auto" style="border-radius: 10px; margin: 5px; object-fit: cover; aspect-ratio: 1/1;" alt='Gallop' onclick="populateDemo(this, 1);">
                  <img src="assets/task_chooser/real_world_5.png" width="8%" height="auto" style="border-radius: 10px; margin: 5px; object-fit: cover; aspect-ratio: 1/1;" alt='Rotate in place' onclick="populateDemo(this, 1);">
                  <img src="assets/task_chooser/real_world_6.png" width="8%" height="auto" style="border-radius: 10px; margin: 5px; object-fit: cover; aspect-ratio: 1/1;" alt='Retrieve from safe' onclick="populateDemo(this, 1);">
                  <img src="assets/task_chooser/block_stacking_1.png" width="8%" height="auto" style="border-radius: 10px; margin: 5px; object-fit: cover; aspect-ratio: 1/1;" alt='Unload Cart' onclick="populateDemo(this, 1);">
                  <img src="assets/task_chooser/block_stacking_2.png" width="8%" height="auto" style="border-radius: 10px; margin: 5px; object-fit: cover; aspect-ratio: 1/1;" alt='Unload Cart' onclick="populateDemo(this, 1);">
                  <img src="assets/task_chooser/block_stacking_3.png" width="8%" height="auto" style="border-radius: 10px; margin: 5px; object-fit: cover; aspect-ratio: 1/1;" alt='Unload Cart' onclick="populateDemo(this, 1);">
                  <img src="assets/task_chooser/shelf_packing.png" width="8%" height="auto" style="border-radius: 10px; margin: 5px; object-fit: cover; aspect-ratio: 1/1;" alt='Unload Cart' onclick="populateDemo(this, 1);">
              </div>
          </div>
          
            <!-- Video Player -->
            <div class="row border rounded" style="padding-top:2px; padding-bottom:12px; margin-top: 20px;">
                <div class="col-md-6" style="text-align: center;">
                    <video id="demo-video-1" style="border-radius: 5px; width: 70%;" autoplay loop muted webkit-playsinline playsinline onclick="setAttribute('controls', 'true');">
                        <source id="expandedImg-1" src="assets/task_chooser/select.mp4" type="video/mp4">
                    </video>
                </div>
            </div>
            
        </div>
    </section>

    <script>
    function populateDemo(img, demoId) {
        const videoSource = document.getElementById("expandedImg-" + demoId);
        const videoElement = document.getElementById("demo-video-" + demoId);
        const textElement = document.getElementById("imgtext-" + demoId);
    
        // Extract the video path based on the image source
        const videoPath = img.src.replace('.png', '.mp4').replace('assets/', 'assets/');
        videoSource.src = videoPath;
        videoElement.load(); // Reload video with new source
    
        // Update the text description
        textElement.innerHTML = img.alt;
    }
    </script>

    <!-- System Overview -->
    <section class="section">
        <div class="container is-max-widescreen">
            <div class="rows">
                <div class="rows is-centered">
                    <div class="row is-full-width">
                        <h2 class="title is-3"><span class="dvima"> SPOT: Search over Point Cloud Object Transformations</span></h2>
                        <p style="font-size: 125%">
                            SPOT solves multi-object rearrangement tasks by planning directly in point cloud space, without relying on privileged information such as ground-truth object states. 
                            It performs A* search over object-wise SE(3) transformations, expanding nodes by sampling actions from learned suggesters that serve as domain-specific priors.
                        </p>
                                                 <br>
                         <div style="display: flex; justify-content: center; gap: 20px; align-items: center;">
                             <video autoplay controls muted loop playsinline height="100%" style="max-width: 50%;">
                                 <source src="assets/1p2b1c_plan_and_exec.mp4"
                                         type="video/mp4">
                               </video>
                               <video autoplay controls muted loop playsinline height="100%" style="max-width: 50%;">
                                 <source src="assets/2p1b1c_plan_and_exec.mp4"
                                         type="video/mp4">
                               </video>
                         </div>
                         <br>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section" style="padding-top: 4px;">
        <div class="container is-max-widescreen">
            <div class="rows">
                <div class="rows is-centered">
                    <div class="row is-full-width">
                        <h3 class="title is-4"><span class="dvima">Plan Graph Visualization</span></h3>
                        <p style="font-size: 125%">
                            Shown below is a visualization of the plan graph for a table bussing task. Each node is a point cloud with cost and heuristic values, expanded using 
                           SE(3) transformations from the learned suggesters. A* finds multimodal solutions: one plan moves the cup aside, stacks the bowls, then returns the cup (3 steps), while another places the cup next to the plate, enabling direct stacking (2 steps). High-collision plans are pruned.
                        </p>
                        <br>
                        <img src="assets/plan_graph.png" class="interpolation-image" alt="Plan Graph"
                            style="display: block; margin-left: auto; margin-right: auto; width: 100%;" />
                        <br>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- System Overview -->
    <section class="section">
        <div class="container is-max-widescreen">
            <div class="rows">
                <div class="rows is-centered">
                    <div class="row is-full-width">
                        <h2 class="title is-3"><span class="dvima"> Method Breakdown: </span></h2>
                        <h3 class="title is-4"><span class="dvima">1. Learned Suggesters</span></h3>
                        <p style="font-size: 125%">
                            
                            To efficiently guide search over continuous state-action transitions, SPOT leverages
                            an <b>object suggester</b> and a <b>placement suggester</b> to answer two high-level questions: <b>(1) Which object should be moved?
                            (2) Where should the object be moved to?</b> <br> Both the suggesters operate on a segmented point cloud observation of the scene.
                            
                            <ul style="font-size: 125%; margin-left: 20px;">
                                <li><strong>Object Suggester:</strong> Predicts a probability distribution over the objects in the scene that can feasibly be moved. We sample from this distribution to select an object to move.</li>
                                <li><strong>Placement Suggester:</strong> Samples potential transformations for that object of where it might be moved next. It generates diverse transformations by iteratively rescoring its latent distribution.</li>
                                <li><strong>Model Deviation Estimator (MDE):</strong> Guides search towards actions that are more physically plausible, by estimating the deviation between the expected and true state.</li>
                            </ul>
                            <span style="font-size: 125%;">Together, these modules enable search-based planning to solve complex rearrangement tasks directly from point cloud observations.</span>

                            </p>
                            <br><br>
                            <video autoplay controls muted loop playsinline height="100%">
                                <source src="assets/Suggester_visualization.mp4"
                                        type="video/mp4">
                            </video>

                            <br><br><br>

                            <h3 class="title is-4"><span class="dvima">2. Guided A* Search</span></h3>
                            <p style="font-size: 125%">
                                Our method takes as input an RGB-D image, object names, and a goal function. From a segmented point cloud, A* search plans over the space of 
                                SE(3) transformations applied to object point clouds until a goal-satisfying configuration is reached. Node expansion samples actions in the form of (1) an object to move and (2) its placement, from the learned suggesters. 
                                The cost function for search combines action cost, collision cost, deviation cost from the model deviation estimator (MDE), and probability cost. 
                                The resulting plan is then given to a robot for execution.
                            
                            </p>
                                <br><br>
                                <img src="assets/Method.jpg" class="interpolation-image" alt="System Overview"
                            style="display: block; margin-left: auto; margin-right: auto; width: 100%;" />
                        
                            <br><br><br>

                            <h3 class="title is-4"><span class="dvima">3. Data Collection</span></h3>
                            <p style="font-size: 125%">
                                
                                We train our suggesters using goal-agnostic, but domain-specific human demonstration videos. 

                                We use <a href="https://github.com/IDEA-Research/Grounded-SAM-2" target="_blank">Grounded Segment Anything</a> 
                                to segment out each object in the video. Then,
                                using <a href="https://cotracker3.github.io/" target="_blank">CoTracker3</a>, we extract 2D tracks across video 
                                frames for each object in the scene. By applying a threshold to the 2D tracks, we identify stationary periods and movement periods for each
                                object in the scene, shown in the graphs below. Assuming that every object moves independently of the other objects, we deter-
                                mine the order of object movements. For each detected movement, RANSAC-based SVD computes
                                the best rigid transformation (4×4 matrices) between stationary states of each object. The output is
                                a sequence of transformations and point clouds describing how objects were manipulated, as shown below. 
                                See <a href="https://github.com/kallol-saha/video_to_transforms" target="_blank">this repository</a> for implementation.

                                <br><br>
                                <div style="text-align: center;">
                                    <video autoplay controls muted loop playsinline height="100%" style="max-width: 70%;">
                                        <source src="assets/data_collection.mp4"
                                                type="video/mp4">
                                    </video>
                                </div>
                                <img src="assets/data_collection.jpg" class="interpolation-image" alt="System Overview"
                            style="display: block; margin-left: auto; margin-right: auto; width: 80%;" />    
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-widescreen">
            <div class="rows">
                <div class="rows is-centered">
                    <div class="row is-full-width">
                        <h2 class="title is-3"><span class="dvima"> Planning and Execution Success Rates</span></h2>
                        <p style="font-size: 125%">

                            <ul style="font-size: 125%; margin-left: 20px; list-style-type: disc;">
                                <li>SPOT outperforms Beam Search with a beam width of 1, highlighting the advantage of using A* search for planning.</li>
                                <li>SPOT outperforms random rollouts (which expand a large number of nodes), demonstrating the importance of guided search toward the goal.</li>
                                <li>SPOT performs better with the object suggester than with uniform object sampling, showing the value of the object suggester as a domain-specific prior.</li>
                                <li>SPOT achieves performance comparable to <a href="https://sites.google.com/stanford.edu/points2plans" target="_blank">Points2Plans</a> and surpasses <a href="https://sites.google.com/view/erelationaldynamics" target="_blank">eRDTransformer</a> and <a href="https://arxiv.org/abs/2108.12062" target="_blank">Pairwise-RD</a>, even though we do not use relational state abstractions or symbolic scene representations. We also plan in the point cloud space instead of the latent space.</li>
                                <li>SPOT outperforms <a href="https://3d-diffusion-policy.github.io/" target="_blank">3D Diffusion Policy</a> (trained on the same data) on long-horizon tasks, demonstrating long-horizon generalization.</li>
                            </ul>


                        </p>
                        <br>
                        <!-- <img src="assets/pipeline.m4v" class="interpolation-image" alt=""
                            style="display: block; margin-left: auto; margin-right: auto; max-width: 80%;" /> -->
                            <img src="assets/results.png" class="interpolation-image" alt="System Overview"
                            style="display: block; margin-left: auto; margin-right: auto; width: 100%;" />
                            
                        <br>
                        </div>
                </div>
            </div>
        </div>
    </section>


    <section class="section">
        <div class="container is-max-widescreen">
            <div class="rows">
                <div class="rows is-centered">
                    <div class="row is-full-width">
                        <h2 class="title is-3"><span class="dvima"> Qualitative Analysis: Long-Horizon Planning</span></h2>
                        <p style="font-size: 125%">We compare the execution performance of our method to <a href="https://3d-diffusion-policy.github.io/" target="_blank">3D Diffusion Policy</a> (DP3), an end-to-end imitation learning policy. 
                            Our DP3 baseline is trained on a dataset of 23 task-specific demonstrations of a simulation block stacking task provided by a human expert. 
                            All of the demonstrations lead to a single consistent goal configuration since DP3 is not goal-conditioned. 
                            As shown below, although DP3 achieves some success on 2-step tasks, it is unable to complete any 3 or 4-step tasks. 
                            <b>This demonstrates that policy learning used directly can have compounding errors in long-horizon tasks, whereas search-based planning guided by learned modules can generalize to longer horizons.</b> See Table 2 in our paper for success rates.</p>
                        <!-- <img src="assets/pipeline.m4v" class="interpolation-image" alt=""
                            style="display: block; margin-left: auto; margin-right: auto; max-width: 80%;" /> -->
                            <br>
                            <div style="display: flex; justify-content: center;">
                                <video autoplay controls muted loop playsinline height="100%" style="width: 110%; max-width: 1100px;">
                                  <source src="assets/long_horizon.mp4"
                                          type="video/mp4">
                                </video>
                            </div>
                        <br>
                        </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-widescreen">
            <div class="rows">
                <div class="rows is-centered">
                    <div class="row is-full-width">
                        <h2 class="title is-3"><span class="dvima"> Qualitative Analysis: Out-performing Human Demonstrations</span></h2>
                        <p style="font-size: 125%">Our method can often complete the task more efficiently than the human demonstrations, 
                            since our learned object and placement suggesters capture a <b>goal-agnostic distribution</b> of relevant objects and placements.
                            By sampling from this distribution, our approach explores different possible solutions using search-based planning, <b>allowing it to find shorter and more effective paths to the goal</b>, compared to the human demonstration. </p>
                        <!-- <img src="assets/pipeline.m4v" class="interpolation-image" alt=""
                            style="display: block; margin-left: auto; margin-right: auto; max-width: 80%;" /> -->
                            <div style="display: flex; justify-content: center;">
                                <video autoplay controls muted loop playsinline height="100%" style="width: 110%; max-width: 1100px;">
                                  <source src="assets/efficient_planning.mp4"
                                          type="video/mp4">
                                </video>
                            </div>
                        <br>
                        </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-widescreen">
            <div class="rows">
                <div class="rows is-centered">
                    <div class="row is-full-width">
                        <h2 class="title is-3"><span class="dvima">Acknowledgements</span></h2>
                        <p style="font-size: 125%">This material is based upon work supported by ONR MURI N00014-24-1-2748 and by the Toyota Research Institute. Any opinions, findings, 
                            and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Office of Naval Research 
                            or Toyota Research Institute. We also thank the members of the R-PAD Lab and SBPL for helpful discussions and constructive feedback on this project.
                        </p>
                        <br>
                        </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-widescreen">
            <div class="rows">
                <div class="rows is-centered">
                    <div class="row is-full-width">
                        <div style="display: flex; justify-content: center;">
                            <a href="https://r-pad.github.io/">
                                <img src="logos/rpad.png" style="width: auto; height: 100px; margin-right: 50px;">
                            </a>
                            <a href="http://www.sbpl.net/Home">
                                <img src="logos/sbpl.png" style="width: auto; height: 100px; margin-right: 50px;">
                            </a>
                            <a href="https://www.onr.navy.mil/">
                                <img src="logos/onr.png" style="width: auto; height: 100px; margin-right: 50px;">
                            </a>
                            <a href="https://www.tri.global/">
                                <img src="logos/tri.svg" style="width: auto; height: 60px; margin-top: 20px;">
                            </a>
                        </div>
                        <br>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section" id="BibTeX">
        <div class="container is-max-widescreen content">
            <div class="rows">
                <div class="rows is-centered">
                    <div class="row is-full-width">
                        <h2 class="title is-3"><span class="dvima">BibTeX</span></h2>
                        <p style="font-size: 125%">
                            <pre>@inproceedings{saha2025planning,
    title={Planning from Point Clouds over Continuous Actions for Multi-object Rearrangement},
    author={Saha, Kallol and Li, Amber and Rodriguez-Izquierdo, Angela and Yu, Lifan and Eisner, Ben and Likhachev, Maxim and Held, David},
    booktitle={Conference on Robot Learning (CoRL)},
    year={2025}
}</pre>
                        </p>
                        </div>
                </div>
            </div>
        </div>
    </section>


<script>

  timeoutIds = [];

  function typeWriter(txt, i, q, num, text1, text2) {
      var imgText = document.getElementById(text1 + num);
      var answer = document.getElementById(text2 + num);
      if (imgText.innerHTML == q) {
          for (let k = 0; k < 5; k++) {
              if (i < txt.length) {
                  if (txt.charAt(i) == "\\") {
                      answer.innerHTML += "\n";
                      i += 1;
                  } else {
                      answer.innerHTML += txt.charAt(i);
                  }
                  i++;
              }
          }
          hljs.highlightAll();
          timeoutIds.push(setTimeout(typeWriter, 1, txt, i, q, num, text1, text2));
      }
  }

</script>

</html>
