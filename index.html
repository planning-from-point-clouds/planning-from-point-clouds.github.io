<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Planning from Point Clouds</title>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="./static/css/academicons.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
</head>

<body>


    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">


                        <h1 class="title is-1 publication-title">
                            Planning from Point Clouds over Continuous Actions for Multi-object Rearrangement
                        </h1>
                        <h3 class="title is-4 conference-authors"><a target="_blank" href="https://roboticsconference.org/">Under Review</a>
                        </h3>

                    <div class="is-size-5 publication-authors">
                        <span class="author-block">
                            <a target="_blank" href="" style="font-size: 115%;">
                                Anonymous Authors</a>

                        </span>
                    </div>

                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <!-- Abstract. -->
            <div class="columns is-centered has-text-centered">
                <div class="column">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p style="font-size: 125%">Multi-object rearrangement is a challenging task that requires robots to reason about a physical 3D scene and the effects of a sequence of actions. While traditional task planning methods are shown to be effective for long-horizon manipulation, they require discretizing the continuous state and action space into symbolic descriptions of objects, object relationships, and actions. Our proposed method is instead able to take in a partially-observed point cloud observation of an initial scene and plan to a goal-satisfying configuration, without needing to discretize the set of actions or object relationships. We formulate the planning problem as an A* search over the space of possible point cloud rearrangements. We sample point cloud transformations from a learned, domain-specific prior and then search for a sequence of such transformations that leads from the initial state to a goal. We evaluate our method in terms of task planning success and task execution success in both simulation and real-world environments. We experimentally demonstrate that our method produces successful plans and outperforms a policy-learning approach; we also perform ablation experiments that show the importance of search in our approach.</p>
<br>
<br>
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Teaser Figure -->
    <section class="section" style="padding: 0">
        <div class="container is-max-desktop">
            <!-- Abstract. -->
            <div class="columns is-centered has-text-centered">
                <video poster="" id="" autoplay muted controls loop width="100%" playbackRate=2.0 style="border-radius: 5px;">
                    <source src="assets/Untitled video - Made with Clipchamp (5).mp4" type="video/mp4">
                </video>
            </div>
        </div>
    </section>

    <!-- System Overview -->
    <section class="section">
        <div class="container is-max-widescreen">
            <div class="rows">
                <div class="rows is-centered">
                    <div class="row is-full-width">
                        <h2 class="title is-3"><span class="dvima"> System Overview</span></h2>
                        <p style="font-size: 125%">Our method takes as input an RGB-D image of the scene, the names of the objects, and a goal function. From this input, it obtains a segmented point cloud, from which A* search begins planning over continuous point cloud space to reach a goal-satisfying configuration. At each step, A* explores possible actions, each consisting of (1) an object to move and (2) a placement for that object. The learned object suggester guides object selection, the predicted object placement is sampled from a learned placement suggester, and the learned model deviation estimator guides search towards actions that are more physically plausible. The method outputs a plan consisting of objects and their transformations, which the robot can execute without additional ground-truth information. </p>
                        <br>
                        <img src="assets/Method.jpg" class="interpolation-image" alt="System Overview"
                            style="display: block; margin-left: auto; margin-right: auto; max-width: 80%;" />
                        <br>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-widescreen">
            <div class="rows">
                <div class="rows is-centered">
                    <div class="row is-full-width">
                        <h2 class="title is-3"><span class="dvima"> Object and Placement Suggesters</span></h2>
                        <p style="font-size: 125%">Learned object and placement suggesters. (a) The object suggester operates on a point cloud observation of the scene. From its
                            outputs, we derive a probability distribution that predicts which objects in the scene can feasibly be moved. (b) Given an object to move and the
                            point cloud of the scene, the placement suggester can sample multiple transformations for that object of where it might be moved next. </p>
                            <br>
                            <img src="assets/Suggester.jpg" class="interpolation-image" alt="System Overview"
                            style="display: block; margin-left: auto; margin-right: auto; width: 50%;" />
                        <br>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-widescreen">
            <div class="rows">
                <div class="rows is-centered">
                    <div class="row is-full-width">
                        <h2 class="title is-3"><span class="dvima"> Data Collection</span></h2>
                        <p style="font-size: 125%">We train our learned placement suggester over demonstrations of each task. 
                            The demonstrations are given in the form of a sequence of RGB-D images recorded from a camera, 
                            together with the semantic names of the objects in the scene. 
                            The initial observation is converted into a segmented point cloud using the extrinsic camera values and <a href="https://github.com/IDEA-Research/Grounded-SAM-2" target="_blank">Grounded Segment Anything</a>. 
                            With a single camera, we can only get partial observations of the scene, which can lead to occlusions. 
                            To avoid this issue, we collect all the subsequent data by transforming each object’s initial point cloud by the 
                            transformation applied at that time step in the demonstration. To track where the objects are moving, we use <a href="https://cotracker3.github.io/" target="_blank">CoTracker3</a>. </p>
                            <br>
                            <video autoplay controls muted loop playsinline height="100%">
                              <source src="assets/Untitled video - Made with Clipchamp (11).mp4"
                                      type="video/mp4">
                            </video>
                        <br>
                        </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-widescreen">
            <div class="rows">
                <div class="rows is-centered">
                    <div class="row is-full-width">
                        <h2 class="title is-3"><span class="dvima"> Comparison to Points2Plans </span></h2>
                        <p style="font-size: 125%">
                            
                            Similar to our approach, prior work such as <a href="https://sites.google.com/stanford.edu/points2plans" target="_blank">Points2Plans</a>
                            plan to solve long-horizon manipulation tasks from partially-observed point clouds.

                            In contrast to our method, Points2Plans and its baselines make use of relational state abstractions to convert the point cloud observations to a symbolic representation of the scene for planning.
                            We compare against these approaches by running our method in their constrained packing environment, 
                            where the robot must place items into a spatially constrained environment (a cupboard) by carefully reasoning about the objects’ planned placement positions. 
                            <!-- Please refer to <a href="https://sites.google.com/stanford.edu/points2plans" target="_blank">Points2Plans</a> for more details on the task. -->
                            
                            Our method slightly outperforms Points2Plans on the constrained packing task, 
                            achieving 100% task planning success and 84% task execution success on an evaluation set consisting of 5 different initial configurations run 
                            with 100 different random seeds each.
                        </p>
                        <br>
                        <!-- <img src="assets/pipeline.m4v" class="interpolation-image" alt=""
                            style="display: block; margin-left: auto; margin-right: auto; max-width: 80%;" /> -->
                            <img src="assets/Points2Plans_figure.png" class="interpolation-image" alt="System Overview"
                            style="display: block; margin-left: auto; margin-right: auto; width: 50%;" />
                            <video autoplay controls muted loop playsinline height="100%">
                              <source src="assets/Points2Plans_video2.mp4"
                                      type="video/mp4">
                            </video>
                        <br>
                        </div>
                </div>
            </div>
        </div>
    </section>


    <section class="section">
        <div class="container is-max-widescreen">
            <div class="rows">
                <div class="rows is-centered">
                    <div class="row is-full-width">
                        <h2 class="title is-3"><span class="dvima"> Simulation Results</span></h2>
                        <p style="font-size: 125%">We compare the execution performance of our method to <a href="https://3d-diffusion-policy.github.io/" target="_blank">3D Diffusion Policy</a>, an end-to-end imitation learning policy. 
                            Our DP3 baseline is trained on a dataset of 23 task-specific demonstrations of a simulation block stacking task provided by a human expert. 
                            All of the demonstrations lead to a single consistent goal configuration since DP3 is not goal-conditioned. 
                            We evaluate on five tasks of varying complexity, where task complexity is defined by the number of steps in an optimal plan to reach a goal state. 
                            As shown below, although DP3 achieves some success on 2-step tasks, it is unable to complete any 3 or 4-step tasks. See Table 2 in our paper for success rates.</p>
                        <!-- <img src="assets/pipeline.m4v" class="interpolation-image" alt=""
                            style="display: block; margin-left: auto; margin-right: auto; max-width: 80%;" /> -->
                            <br>
                            <video autoplay controls muted loop playsinline height="100%">
                              <source src="assets/Untitled video - Made with Clipchamp (7).mp4"
                                      type="video/mp4">
                            </video>
                        <br>
                        </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-widescreen">
            <div class="rows">
                <div class="rows is-centered">
                    <div class="row is-full-width">
                        <h2 class="title is-3"><span class="dvima"> Qualitative Analysis: Finding Efficient Path Lengths</span></h2>
                        <p style="font-size: 125%">Our method can often complete the task more efficiently than the human demonstrations, 
                            since our learned object and placement suggesters capture a goal-agnostic distribution of relevant objects and placements.
                            By sampling from this distribution, our approach explores different possible solutions, allowing it to find shorter and more effective paths to the goal, compared to the human demonstration. </p>
                        <!-- <img src="assets/pipeline.m4v" class="interpolation-image" alt=""
                            style="display: block; margin-left: auto; margin-right: auto; max-width: 80%;" /> -->
                            <video autoplay controls muted loop playsinline height="100%">
                              <source src="assets/Untitled video - Made with Clipchamp (6).mp4"
                                      type="video/mp4">
                            </video>
                        <br>
                        </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-widescreen">
            <div class="rows">
                <div class="rows is-centered">
                    <div class="row is-full-width">
                        <h2 class="title is-3"><span class="dvima"> Generalization to unseen Configurations</span></h2>
                        <p style="font-size: 125%">Our method generalizes to previously unseen configurations and successfully finds optimal paths for them as well. This demonstrates that
                            the object and placement suggesters have learned a generalizable distribution that is leveraged by search to find optimal paths.
                        </p>
                        <br>
                        <!-- <img src="assets/pipeline.m4v" class="interpolation-image" alt=""
                            style="display: block; margin-left: auto; margin-right: auto; max-width: 80%;" /> -->
                            <video autoplay controls muted loop playsinline height="100%">
                              <source src="assets/Untitled video - Made with Clipchamp (10).mp4"
                                      type="video/mp4">
                            </video>
                        <br>
                        </div>
                </div>
            </div>
        </div>
    </section>


<script>

  timeoutIds = [];

  function typeWriter(txt, i, q, num, text1, text2) {
      var imgText = document.getElementById(text1 + num);
      var answer = document.getElementById(text2 + num);
      if (imgText.innerHTML == q) {
          for (let k = 0; k < 5; k++) {
              if (i < txt.length) {
                  if (txt.charAt(i) == "\\") {
                      answer.innerHTML += "\n";
                      i += 1;
                  } else {
                      answer.innerHTML += txt.charAt(i);
                  }
                  i++;
              }
          }
          hljs.highlightAll();
          timeoutIds.push(setTimeout(typeWriter, 1, txt, i, q, num, text1, text2));
      }
  }

</script>

</html>
